name: Quality Check

on:
  push:
    branches: ["main", "develop"]
  pull_request:
    branches: ["main", "develop"]

permissions:
  contents: read

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    outputs:
      eslint_errors: ${{ steps.eslint.outputs.errors }}
      eslint_warnings: ${{ steps.eslint.outputs.warnings }}
      ts_errors: ${{ steps.typecheck.outputs.errors }}
      tests_passed: ${{ steps.tests.outputs.passed }}
      rustfmt_ok: ${{ steps.rustfmt.outputs.fmt_ok }}
      clippy_errors: ${{ steps.clippy.outputs.errors }}
      clippy_warnings: ${{ steps.clippy.outputs.warnings }}
      score: ${{ steps.score.outputs.score }}
      grade: ${{ steps.score.outputs.grade }}
      color: ${{ steps.score.outputs.color }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: latest

      - name: Install Dependencies
        run: cd app && bun install

      # â”€â”€ TypeScript â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Type Check
        id: typecheck
        run: |
          set +e
          cd app
          tsc_out=$(bun run type-check 2>&1)
          echo "$tsc_out"
          # Count error lines (lines containing "error TS")
          err_count=$(echo "$tsc_out" | grep -c "error TS" || true)
          err_count=$(echo "$err_count" | tr -d '\n' | head -1)
          echo "errors=$err_count" >> "$GITHUB_OUTPUT"
          echo "ðŸ“Š TypeScript: $err_count error(s)"

      # â”€â”€ ESLint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Lint (ESLint)
        id: eslint
        run: |
          set +e
          cd app
          # Run ESLint with JSON format, capture output â€“ never fail the step
          eslint_json=$(bun run lint -- --format json 2>/dev/null || true)

          # Fall back: if JSON is empty/invalid, run plain and count manually
          if echo "$eslint_json" | node -e "JSON.parse(require('fs').readFileSync('/dev/stdin','utf8'))" 2>/dev/null; then
            errors=$(echo "$eslint_json" | node -e "
              const d=JSON.parse(require('fs').readFileSync('/dev/stdin','utf8'));
              console.log(d.reduce((s,f)=>s+f.errorCount,0));
            ")
            warnings=$(echo "$eslint_json" | node -e "
              const d=JSON.parse(require('fs').readFileSync('/dev/stdin','utf8'));
              console.log(d.reduce((s,f)=>s+f.warningCount,0));
            ")
          else
            # Plain text fallback
            plain_out=$(bun run lint 2>&1 || true)
            echo "$plain_out"
            errors=$(echo "$plain_out" | grep -c "error" | head -1 || echo "0")
            warnings=$(echo "$plain_out" | grep -c "warning" | head -1 || echo "0")
          fi

          # Sanitize counts
          errors=$(echo "${errors:-0}" | tr -d '\n' | head -1)
          warnings=$(echo "${warnings:-0}" | tr -d '\n' | head -1)

          echo "errors=${errors}" >> "$GITHUB_OUTPUT"
          echo "warnings=${warnings}" >> "$GITHUB_OUTPUT"
          echo "ðŸ“Š ESLint: ${errors} error(s), ${warnings} warning(s)"

      # â”€â”€ Unit Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Unit Tests
        id: tests
        run: |
          set +e
          cd app
          bun run test
          exit_code=$?
          if [ $exit_code -eq 0 ]; then
            echo "passed=true" >> "$GITHUB_OUTPUT"
            echo "âœ… Tests passed"
          else
            echo "passed=false" >> "$GITHUB_OUTPUT"
            echo "ðŸ“Š Tests failed (score will be affected)"
          fi

      # â”€â”€ Server Build â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Build Server Binary (Linux host)
        run: cd app && bun build src/index.ts --compile --target bun-linux-x64 --outfile src-tauri/bin/zentrio-server-x86_64-unknown-linux-gnu

      # â”€â”€ Rust â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy, rustfmt

      - name: Install System Dependencies
        run: sudo apt-get update && sudo apt-get install -y libglib2.0-dev libgtk-3-dev libwebkit2gtk-4.1-dev libayatana-appindicator3-dev librsvg2-dev

      - name: Rust Cache
        uses: swatinem/rust-cache@v2
        with:
          workspaces: "./app/src-tauri -> target"

      - name: Rust Formatting
        id: rustfmt
        run: |
          set +e
          cd app/src-tauri
          cargo fmt -- --check
          fmt_exit=$?
          if [ $fmt_exit -eq 0 ]; then
            echo "fmt_ok=true" >> "$GITHUB_OUTPUT"
            echo "âœ… Rust formatting OK"
          else
            echo "fmt_ok=false" >> "$GITHUB_OUTPUT"
            echo "ðŸ“Š Rust formatting issues found (score will be affected)"
          fi

      - name: Rust Linting (Clippy)
        id: clippy
        run: |
          set +e
          cd app/src-tauri
          # Use -W warnings so warnings are reported but don't fail
          clippy_out=$(cargo clippy -- -W warnings 2>&1)
          echo "$clippy_out"
          warn_count=$(echo "$clippy_out" | grep -c "^warning" | head -1 || echo "0")
          err_count=$(echo "$clippy_out" | grep -c "^error" | head -1 || echo "0")
          # Sanitize to ensure single integer values
          warn_count=$(echo "$warn_count" | tr -d '\n' | head -1)
          err_count=$(echo "$err_count" | tr -d '\n' | head -1)
          echo "warnings=${warn_count}" >> "$GITHUB_OUTPUT"
          echo "errors=${err_count}" >> "$GITHUB_OUTPUT"
          echo "ðŸ“Š Clippy: ${err_count} error(s), ${warn_count} warning(s)"

      # â”€â”€ Score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Calculate Quality Score
        id: score
        if: always()
        run: |
          # Inputs (default to 0 if not available)
          TS_ERRORS="${{ steps.typecheck.outputs.errors || '0' }}"
          ESLINT_ERRORS="${{ steps.eslint.outputs.errors || '0' }}"
          ESLINT_WARNINGS="${{ steps.eslint.outputs.warnings || '0' }}"
          TESTS_PASSED="${{ steps.tests.outputs.passed || 'false' }}"
          CLIPPY_ERRORS="${{ steps.clippy.outputs.errors || '0' }}"
          CLIPPY_WARNINGS="${{ steps.clippy.outputs.warnings || '0' }}"
          RUSTFMT_OK="${{ steps.rustfmt.outputs.fmt_ok || 'false' }}"

          # Scoring (100 points, deductions):
          #   TypeScript errors:   -10 per error (max -50)
          #   TypeScript warnings: -2 per warning (max -20)
          #   ESLint errors:       -10 per error (max -50)
          #   ESLint warnings:     -3 per warning (max -30)
          #   Tests failed:        -25
          #   Rust fmt issues:     -10
          #   Clippy errors:       -10 per error (max -30)
          #   Clippy warnings:     -3 per warning (max -20)

          python3 - <<'PYEOF'
          import os, math

          ts_err   = int(os.environ.get('TS_ERRORS', '0'))
          esl_err  = int(os.environ.get('ESLINT_ERRORS', '0'))
          esl_warn = int(os.environ.get('ESLINT_WARNINGS', '0'))
          tests_ok = os.environ.get('TESTS_PASSED', 'false') == 'true'
          clip_err = int(os.environ.get('CLIPPY_ERRORS', '0'))
          clip_w   = int(os.environ.get('CLIPPY_WARNINGS', '0'))
          rustfmt_ok = os.environ.get('RUSTFMT_OK', 'false') == 'true'

          score = 100
          score -= min(ts_err   * 10, 50)
          score -= min(esl_err  * 10, 50)
          score -= min(esl_warn * 3,  30)
          if not tests_ok:
              score -= 25
          if not rustfmt_ok:
              score -= 10
          score -= min(clip_err * 10, 30)
          score -= min(clip_w   * 3,  20)

          score = max(0, score)

          if score >= 90:
              grade, color = "A", "brightgreen"
          elif score >= 80:
              grade, color = "B", "green"
          elif score >= 70:
              grade, color = "C", "yellowgreen"
          elif score >= 60:
              grade, color = "D", "yellow"
          elif score >= 40:
              grade, color = "E", "orange"
          else:
              grade, color = "F", "red"

          print(f"Score: {score}/100  Grade: {grade}  Color: {color}")

          # Write to GITHUB_OUTPUT
          with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
              fh.write(f"score={score}\n")
              fh.write(f"grade={grade}\n")
              fh.write(f"color={color}\n")
          PYEOF

      - name: Save quality metrics
        if: always() && github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          mkdir -p .github/badges
          cat > .github/badges/quality-metrics.json << 'EOF'
          {
            "score": "${{ steps.score.outputs.score }}",
            "grade": "${{ steps.score.outputs.grade }}",
            "color": "${{ steps.score.outputs.color }}",
            "ts_errors": "${{ steps.typecheck.outputs.errors }}",
            "eslint_errors": "${{ steps.eslint.outputs.errors }}",
            "eslint_warnings": "${{ steps.eslint.outputs.warnings }}",
            "tests_passed": "${{ steps.tests.outputs.passed }}",
            "rustfmt_ok": "${{ steps.rustfmt.outputs.fmt_ok }}",
            "clippy_errors": "${{ steps.clippy.outputs.errors }}",
            "clippy_warnings": "${{ steps.clippy.outputs.warnings }}",
            "run_id": "${{ github.run_id }}"
          }
          EOF
          cat .github/badges/quality-metrics.json

      - name: Upload quality metrics artifact
        if: always() && github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v4
        with:
          name: quality-metrics
          path: .github/badges/quality-metrics.json
          retention-days: 1
          overwrite: true

      - name: Print score summary
        if: always()
        run: |
          echo "## ðŸ“Š Quality Score" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "| Metric | Value |" >> "$GITHUB_STEP_SUMMARY"
          echo "|--------|-------|" >> "$GITHUB_STEP_SUMMARY"
          echo "| **Score** | **${{ steps.score.outputs.score }}/100** |" >> "$GITHUB_STEP_SUMMARY"
          echo "| **Grade** | **${{ steps.score.outputs.grade }}** |" >> "$GITHUB_STEP_SUMMARY"
          echo "| TypeScript errors | ${{ steps.typecheck.outputs.errors }} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| ESLint errors | ${{ steps.eslint.outputs.errors }} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| ESLint warnings | ${{ steps.eslint.outputs.warnings }} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Tests | ${{ steps.tests.outputs.passed == 'true' && 'âœ… passed' || 'âŒ failed' }} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Rust fmt | ${{ steps.rustfmt.outputs.fmt_ok == 'true' && 'âœ… OK' || 'âŒ issues' }} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Clippy errors | ${{ steps.clippy.outputs.errors }} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Clippy warnings | ${{ steps.clippy.outputs.warnings }} |" >> "$GITHUB_STEP_SUMMARY"
